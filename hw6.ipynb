{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6289c3",
   "metadata": {},
   "source": [
    "# Автоматическая компрессия моделей (Optimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "09a1c33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from itertools import islice\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "from optimum.pipelines import pipeline as optimum_pipeline\n",
    "\n",
    "from transformers import pipeline as transformers_pipeline\n",
    "from optimum.onnxruntime import ORTModelForSpeechSeq2Seq, ORTQuantizer, AutoQuantizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8b6f5c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 50\n",
    "librispeech_test_clean = load_dataset(\"librispeech_asr\", \"clean\", split=f\"test[:{NUM_EXAMPLES}]\")\n",
    "warmup_samples = islice(librispeech_test_clean, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93d26d",
   "metadata": {},
   "source": [
    "## Базовая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a4c20ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c39ecac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_pred(batch, model):\n",
    "    audio = batch[\"audio\"]\n",
    "    input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features.to(\"cpu\"))[0]\n",
    "    transcription = processor.decode(predicted_ids)\n",
    "    return processor.tokenizer._normalize(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15df005b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [02:13<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time on 50 examples: 133.24286723136902\n",
      "avg. time on example: 2.6648573446273804\n",
      "WER: 3.982777179763186\n",
      "CER: 1.340373679935012\n"
     ]
    }
   ],
   "source": [
    "res = defaultdict(list)\n",
    "\n",
    "#WARMUP\n",
    "for el in tqdm(warmup_samples):\n",
    "    map_to_pred(el, model)\n",
    "\n",
    "#INFERENCE\n",
    "t = time()\n",
    "for el in tqdm(librispeech_test_clean):\n",
    "    res[\"reference\"].append(processor.tokenizer._normalize(el['text']))\n",
    "    res[\"prediction\"].append(map_to_pred(el, model))\n",
    "\n",
    "total_t = time() - t\n",
    "t = (total_t / NUM_EXAMPLES)\n",
    "print(f\"total time on 50 examples: {total_t}\")\n",
    "print(f\"avg. time on example: {t}\")\n",
    "\n",
    "wer = load(\"wer\")\n",
    "print(f\"WER: {100 * wer.compute(references=res['reference'], predictions=res['prediction'])}\")\n",
    "\n",
    "cer = load(\"cer\")\n",
    "print(f\"CER: {100 * cer.compute(references=res['reference'], predictions=res['prediction'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561aae4d",
   "metadata": {},
   "source": [
    "## Inference pipelines with the ONNX Runtime accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a590cd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using the export variant default. Available variants are:\n",
      "\t- default: The default ONNX variant.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using framework PyTorch: 2.1.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:410: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:449: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "Using framework PyTorch: 2.1.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:1004: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:417: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "Using framework PyTorch: 2.1.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "Asked a sequence length of 16, but a sequence length of 1 will be used with use_past == True for `decoder_input_ids`.\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:372: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "onnx_asr_pipe = optimum_pipeline(\n",
    "  \"automatic-speech-recognition\",\n",
    "  model=\"openai/whisper-small\",\n",
    "  accelerator=\"ort\",\n",
    "  #device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cba0179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = onnx_asr_pipe.tokenizer.feature_extractor\n",
    "tokenizer = onnx_asr_pipe.tokenizer.tokenizer\n",
    "\n",
    "onnx_asr_pipe.feature_extractor = feature_extractor\n",
    "onnx_asr_pipe.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "25586e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [01:34<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time on 50 examples: 94.12685203552246\n",
      "avg. time on example: 1.8825370407104491\n",
      "WER: 3.982777179763186\n",
      "CER: 1.340373679935012\n"
     ]
    }
   ],
   "source": [
    "res = defaultdict(list)\n",
    "\n",
    "#WARMUP\n",
    "for el in tqdm(warmup_samples):\n",
    "    onnx_asr_pipe(el[\"audio\"][\"array\"])\n",
    "\n",
    "#INFERENCE\n",
    "t = time()\n",
    "for el in tqdm(librispeech_test_clean):\n",
    "    #print(el)\n",
    "    res[\"reference\"].append(processor.tokenizer._normalize(el['text']))\n",
    "    res[\"prediction\"].append(processor.tokenizer._normalize(onnx_asr_pipe(el[\"audio\"][\"array\"])['text']))\n",
    "\n",
    "total_t = time() - t\n",
    "t = (total_t / NUM_EXAMPLES)\n",
    "print(f\"total time on 50 examples: {total_t}\")\n",
    "print(f\"avg. time on example: {t}\")\n",
    "\n",
    "wer = load(\"wer\")\n",
    "print(f\"WER: {100 * wer.compute(references=res['reference'], predictions=res['prediction'])}\")\n",
    "\n",
    "cer = load(\"cer\")\n",
    "print(f\"CER: {100 * cer.compute(references=res['reference'], predictions=res['prediction'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c153865a",
   "metadata": {},
   "source": [
    "## Optimum Inference with ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40e48526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using the export variant default. Available variants are:\n",
      "\t- default: The default ONNX variant.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using framework PyTorch: 2.1.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:410: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:449: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "Using framework PyTorch: 2.1.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:1004: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:417: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "Using framework PyTorch: 2.1.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "Asked a sequence length of 16, but a sequence length of 1 will be used with use_past == True for `decoder_input_ids`.\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:372: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = ORTModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-small\", export=True) # ONNX checkpoint\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "onnx_asr = transformers_pipeline(\"automatic-speech-recognition\", model=model, \n",
    "                                 tokenizer=processor.tokenizer, feature_extractor=processor.feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8588d55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [01:31<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time on 50 examples: 91.01328778266907\n",
      "avg. time on example: 1.8202657556533814\n",
      "WER: 3.982777179763186\n",
      "CER: 1.340373679935012\n"
     ]
    }
   ],
   "source": [
    "res = defaultdict(list)\n",
    "\n",
    "#WARMUP\n",
    "for el in tqdm(warmup_samples):\n",
    "    onnx_asr(el[\"audio\"][\"array\"])\n",
    "\n",
    "#INFERENCE\n",
    "t = time()\n",
    "for el in tqdm(librispeech_test_clean):\n",
    "    #print(el)\n",
    "    res[\"reference\"].append(processor.tokenizer._normalize(el['text']))\n",
    "    res[\"prediction\"].append(processor.tokenizer._normalize(onnx_asr(el[\"audio\"][\"array\"])['text']))\n",
    "\n",
    "total_t = time() - t\n",
    "t = (total_t / NUM_EXAMPLES)\n",
    "print(f\"total time on 50 examples: {total_t}\")\n",
    "print(f\"avg. time on example: {t}\")\n",
    "\n",
    "wer = load(\"wer\")\n",
    "print(f\"WER: {100 * wer.compute(references=res['reference'], predictions=res['prediction'])}\")\n",
    "\n",
    "cer = load(\"cer\")\n",
    "print(f\"CER: {100 * cer.compute(references=res['reference'], predictions=res['prediction'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4cd2f",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22723d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using the export variant default. Available variants are:\n",
      "\t- default: The default ONNX variant.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using framework PyTorch: 2.1.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:410: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:449: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n",
      "Using framework PyTorch: 2.1.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:1004: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:417: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n",
      "Using framework PyTorch: 2.1.0\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "Asked a sequence length of 16, but a sequence length of 1 will be used with use_past == True for `decoder_input_ids`.\n",
      "C:\\Users\\Arina\\anaconda3\\envs\\CompressionCourse\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:372: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  and past_key_value[0].shape[2] == key_value_states.shape[1]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Arina/AppData/Local/Temp/tmp00i4nwai')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model = ORTModelForSpeechSeq2Seq.from_pretrained(\"openai/whisper-small\", export=True)\n",
    "model_dir = onnx_model.model_save_dir\n",
    "model_dir\n",
    "#quantizer = ORTQuantizer.from_pretrained(ort_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d0a84db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder quantizer\n",
    "encoder_quantizer = ORTQuantizer.from_pretrained(model_dir, file_name=\"encoder_model.onnx\")\n",
    "\n",
    "# Create decoder quantizer\n",
    "decoder_quantizer = ORTQuantizer.from_pretrained(model_dir, file_name=\"decoder_model.onnx\")\n",
    "\n",
    "# Create decoder with past key values quantizer\n",
    "decoder_wp_quantizer = ORTQuantizer.from_pretrained(model_dir, file_name=\"decoder_with_past_model.onnx\")\n",
    "\n",
    "# Create Quantizer list\n",
    "quantizer = [encoder_quantizer, decoder_quantizer, decoder_wp_quantizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ef78fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqconfig = AutoQuantizationConfig.avx2(is_static=False, per_channel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a7964e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/u8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: quantized_model (external data format: False)\n",
      "Configuration saved in quantized_model\\ort_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/u8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: quantized_model (external data format: False)\n",
      "Configuration saved in quantized_model\\ort_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Creating dynamic quantizer: QOperator (mode: IntegerOps, schema: u8/u8, channel-wise: False)\n",
      "Quantizing model...\n",
      "Saving quantized model at: quantized_model (external data format: False)\n",
      "Configuration saved in quantized_model\\ort_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "for q in quantizer:\n",
    "    q.quantize(save_dir=\"./quantized_model\",quantization_config=dqconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b94c6fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    }
   ],
   "source": [
    "quantized_model = ORTModelForSpeechSeq2Seq.from_pretrained('./quantized_model')\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "onnx_quantized_asr = transformers_pipeline(\"automatic-speech-recognition\", model=quantized_model, \n",
    "                                 tokenizer=processor.tokenizer, feature_extractor=processor.feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "46e1988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [01:19<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time on 50 examples: 79.68720746040344\n",
      "avg. time on example: 1.5937441492080688\n",
      "WER: 4.090419806243272\n",
      "CER: 1.421608448415922\n"
     ]
    }
   ],
   "source": [
    "res = defaultdict(list)\n",
    "\n",
    "#WARMUP\n",
    "for el in tqdm(warmup_samples):\n",
    "    onnx_asr(el[\"audio\"][\"array\"])\n",
    "\n",
    "#INFERENCE\n",
    "t = time()\n",
    "for el in tqdm(librispeech_test_clean):\n",
    "    #print(el)\n",
    "    res[\"reference\"].append(processor.tokenizer._normalize(el['text']))\n",
    "    res[\"prediction\"].append(processor.tokenizer._normalize(onnx_quantized_asr(el[\"audio\"][\"array\"])['text']))\n",
    "\n",
    "total_t = time() - t\n",
    "t = (total_t / NUM_EXAMPLES)\n",
    "print(f\"total time on 50 examples: {total_t}\")\n",
    "print(f\"avg. time on example: {t}\")\n",
    "\n",
    "wer = load(\"wer\")\n",
    "print(f\"WER: {100 * wer.compute(references=res['reference'], predictions=res['prediction'])}\")\n",
    "\n",
    "cer = load(\"cer\")\n",
    "print(f\"CER: {100 * cer.compute(references=res['reference'], predictions=res['prediction'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95313aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
